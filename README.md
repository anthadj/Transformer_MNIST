# ðŸš€ Transformer_AH

A custom Transformer model implemented in PyTorch, designed for flexibility in encoder-decoder architectures.  
Includes separate modules for:
- Encoder
- Masked decoder
- Encoder-decoder cross-attention combiner

Supports easy switching between encoder-only, decoder-only, and full transformer modes.

---

## ðŸ“‚ Example files

âœ… **`example1-Simple_Initiation.py`**  
A minimal example showing how to:
- Instantiate the model
- Run data through encoder, decoder, or full transformer modes

âœ… **`example2-MNIST_Transformer.py`**  
A more complete example where:
- MNIST image data is processed
- The transformer model is trained on MNIST as input
- Demonstrates how to integrate the model into a training loop

---

## How to use

from transformer_ah import Transformer_AH

model = Transformer_AH(
    enc_emb_dim=128,
    enc_heads=8,
    enc_layers=2,
    ff_dim=256,
    token_feature_dim=196,
    vocab_size=12,
    dec_emb_dim=128,
    dec_heads=8,
    dec_layers=2,
    transformer_emb_dim=128,
    comb_heads=8,
    comb_layers=2
)

## Instantiate the model
python
Copy
Edit
from transformer_ah import Transformer_AH

model = Transformer_AH(
    enc_emb_dim=128,
    enc_heads=8,
    enc_layers=2,
    ff_dim=256,
    token_feature_dim=196,
    vocab_size=12,
    dec_emb_dim=128,
    dec_heads=8,
    dec_layers=2,
    transformer_emb_dim=128,
    comb_heads=8,
    comb_layers=2
)


## Run encoder, decoder, or transformer modes
python
Copy
Edit
import torch

# Example encoder input: (batch_size, seq_len, token_feature_dim)
enc_in = torch.randn(4, 6, 196)

# Example decoder input: (batch_size, seq_len)
dec_in = torch.randint(0, 12, (4, 5))

# Encoder only
enc_out = model(enc_in=enc_in, mode="encoder")

# Decoder only
dec_out = model(dec_in=dec_in, mode="decoder")

# Full transformer
full_out = model(enc_in=enc_in, dec_in=dec_in, mode="transformer")
Example commands
Run the simple initiation example:

bash
Copy
Edit
python example1-Simple_Initiation.py
Run the MNIST transformer example:

bash
Copy
Edit
python example2-MNIST_Transformer.py
Notes
Ensure that your encoder inputâ€™s last dimension matches token_feature_dim.

Decoder inputs must contain token indices in the range [0, vocab_size - 1].










